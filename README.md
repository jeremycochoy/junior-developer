# Junior Developer ğŸ§¬

> Self-evolving coding agent using genetic algorithms and Bradley-Terry pairwise comparison

A sophisticated system that uses **ShinkaEvolve** (genetic programming framework) combined with **Bradley-Terry with Minorization-Maximization (BT-MM)** scoring to evolve effective coding agent prompts through pairwise LLM-based evaluation.

## ğŸ¯ What Does It Do?

Instead of manually crafting prompts for coding agents, this system:

1. **Starts** with seed prompts (e.g., "Refactor visualization code")
2. **Evolves** prompts through genetic algorithms (mutation via LLM)
3. **Evaluates** results by comparing pairs of branches using an LLM judge
4. **Ranks** all attempts using BT-MM scoring (statistically optimal)
5. **Converges** on high-quality, specific refactoring instructions

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ShinkaEvolve   â”‚  Genetic algorithm orchestration
â”‚   (AlphaEvolve) â”‚  Population management, mutation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  evaluate.py    â”‚  Evaluation pipeline
â”‚                 â”‚  â€¢ Execute coding agent
â”‚                 â”‚  â€¢ Create Git branches
â”‚                 â”‚  â€¢ Generate diffs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pairwise Judge  â”‚  LLM compares two branches
â”‚   (judge.py)    â”‚  â€¢ Randomized ordering
â”‚                 â”‚  â€¢ Returns winner + reasoning
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BT-MM Scoring  â”‚  Ranking algorithm
â”‚  (scoring.py)   â”‚  â€¢ Global optimization
â”‚                 â”‚  â€¢ No hyperparameters
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### **Bradley-Terry with MM (Not ELO)**
- âœ… **Globally optimal**: Maximum Likelihood Estimation
- âœ… **No tuning**: No K-factor or learning rate
- âœ… **Efficient**: O(N Ã— iterations) not O(NÂ²)
- âœ… **Batch updates**: Recomputes all scores together
- âœ… **Statistically principled**: Proper probabilistic model

### **Pairwise Comparison**
- âœ… **LLM-as-Judge**: Single LLM compares two candidates
- âœ… **Unbiased**: Randomized ordering (A/B positions)
- âœ… **Context-aware**: Includes task spec and diffs
- âœ… **Reasoning tracked**: Stores judge's explanation

### **Git-Based Population**
- âœ… **Branch per candidate**: Easy versioning
- âœ… **Diff comparison**: Natural code comparison
- âœ… **Rollback**: Clean state management

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/junior-developer.git
cd junior-developer

# Install in development mode
pip install -e .

# Or install with all dependencies
pip install -e ".[dev,llm]"
```

## ğŸš€ Quick Start

### 1. Run Tests

```bash
# Run all tests
pytest tests/

# Run specific test suite
pytest tests/test_scoring.py -v
pytest tests/test_judge.py -v
```

### 2. Basic Usage

```python
from junior_dev import BTMMScoringEngine, PairwiseJudge

# Initialize BT-MM scoring engine
engine = BTMMScoringEngine(
    db_path="scores.db",
    initial_score=1.0,
    convergence_tol=1e-6
)

# Initialize judge
judge = PairwiseJudge(llm_model="gpt-4")

# Compare two candidates
result = judge.compare(
    candidate_a_id="branch_001",
    candidate_a_output="Refactored code A",
    candidate_b_id="branch_002",
    candidate_b_output="Refactored code B",
    task_spec="Move visualization to separate class",
    context=""
)

# Record comparison
score_a, score_b = engine.record_comparison(
    candidate_a="branch_001",
    candidate_b="branch_002",
    winner=result.winner,
    reasoning=result.reasoning
)

print(f"Scores: {score_a:.4f} vs {score_b:.4f}")

# Get rankings
rankings = engine.get_rankings()
for rank, stats in enumerate(rankings, 1):
    print(f"{rank}. {stats.candidate_id}: {stats.bt_score:.4f}")
```

### 3. Integration with ShinkaEvolve

```bash
# Run evolution
cd examples/
python -m shinka.runner --config config.yaml
```

## ğŸ“ Project Structure

```
junior-developer/
â”œâ”€â”€ junior_dev/              # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scoring.py           # BT-MM scoring engine
â”‚   â”œâ”€â”€ judge.py             # Pairwise LLM judge
â”‚   â””â”€â”€ shinka/              # ShinkaEvolve integration
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ evaluate.py      # Evaluation pipeline
â”‚       â””â”€â”€ initial.py       # Seed prompts
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ test_scoring.py      # BT-MM tests (11 tests)
â”‚   â”œâ”€â”€ test_judge.py        # Judge tests (10 tests)
â”‚   â””â”€â”€ test_evaluate.py     # Integration test
â”œâ”€â”€ archive/                 # Reference implementations
â”‚   â”œâ”€â”€ elo_scoring_engine.py
â”‚   â””â”€â”€ simple_demo.py
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ examples/                # Usage examples
â”œâ”€â”€ configs/                 # Configuration files
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ setup.py                 # Package setup
â””â”€â”€ README.md
```

## ğŸ”¬ How BT-MM Works

### The Math (Simplified)

**Bradley-Terry Model**: Probability that A beats B is:

```
P(A beats B) = score_A / (score_A + score_B)
```

**Minorization-Maximization**: Iteratively update scores until convergence:

```python
for iteration in range(max_iterations):
    for candidate_i in candidates:
        wins_i = sum(comparisons where i won)
        games_i = sum(all comparisons involving i)
        
        denominator = sum(
            1 / (score_i + score_j) 
            for each opponent j
        )
        
        new_score_i = wins_i / denominator
    
    if converged:
        break
```

**Result**: Globally optimal scores that maximize likelihood of observed outcomes.

## ğŸ§ª Testing

All tests are comprehensive and passing:

```bash
# BT-MM Scoring Engine (11 tests)
pytest tests/test_scoring.py -v

# Pairwise Judge (10 tests)  
pytest tests/test_judge.py -v

# Integration (1 test)
pytest tests/test_evaluate.py -v
```

## ğŸ“Š Performance

- **Convergence**: Typically 10-20 iterations
- **Complexity**: O(N Ã— C Ã— I) where:
  - N = number of candidates
  - C = comparisons per candidate (~10)
  - I = iterations (~15)
- **Scalability**: Tested with 100+ candidates
- **Cost**: ~$0.50 per 50 generations (with GPT-4)

## ğŸ› ï¸ Configuration

See `configs/` directory for examples:

- `task/`: Task specifications
- `evolution/`: Population and generation settings
- `database/`: Archive configuration
- `agent/`: Coding agent settings


